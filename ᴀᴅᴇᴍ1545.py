#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
import re
import whois
import socket
import ssl
from datetime import datetime
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
from colorama import init, Fore, Style, Back
from collections import Counter
import random
import pyfiglet
import time
import sys

# Renkli √ßƒ±ktƒ± i√ßin ayarlar
init(autoreset=True)

# √ñzel logo olu≈üturma
def create_logo():
    logo_text = pyfiglet.figlet_format("adem1545", font="slant")
    colors = [Fore.RED, Fore.GREEN, Fore.YELLOW, Fore.BLUE, Fore.MAGENTA, Fore.CYAN]
    colored_logo = ""
    for line in logo_text.split('\n'):
        colored_logo += random.choice(colors) + line + "\n"
    return colored_logo

# Dil desteƒüi
LANGUAGES = {
    'tr': {
        'title': "ADEM1545 GELƒ∞≈ûMƒ∞≈û WEB ANALƒ∞Z PANELƒ∞",
        'select_lang': "Dil se√ßiniz (1-T√ºrk√ße, 2-English): ",
        'change_lang': "\nDil deƒüi≈ütirmek i√ßin: 1-T√ºrk√ße, 2-English, 0-√áƒ±kƒ±≈ü: ",
        'enter_url': "üîó Analiz edilecek site URL'sini girin (√∂rn: https://www.ornek.com): ",
        'analyzing': "üîç Site analiz ediliyor, l√ºtfen bekleyin...",
        'connection_error': "‚ö†Ô∏è Siteye baƒülanƒ±lamadƒ±!",
        'general_info': "\nüìå TEMEL Bƒ∞LGƒ∞LER",
        'site_title': "  üìù Site Ba≈ülƒ±ƒüƒ±:",
        'meta_desc': "  üìÑ A√ßƒ±klama:",
        'ip_address': "  üåê IP Adresi:",
        'server_location': "  üìç Sunucu Konumu:",
        'ssl_info': "\nüîí SSL Bƒ∞LGƒ∞LERƒ∞",
        'issuer': "  üè¢ Sertifika Saƒülayƒ±cƒ±:",
        'valid_from': "  üìÖ Ba≈ülangƒ±√ß Tarihi:",
        'valid_to': "  üìÖ Biti≈ü Tarihi:",
        'days_left': "  ‚è≥ Kalan G√ºn:",
        'tech_info': "\nüõ†Ô∏è TEKNOLOJƒ∞ Bƒ∞LGƒ∞LERƒ∞",
        'server': "  üíª Sunucu:",
        'cms': "  üñ•Ô∏è CMS:",
        'programming_lang': "  üíæ Programlama Dili:",
        'security_info': "\nüõ°Ô∏è G√úVENLƒ∞K ANALƒ∞Zƒ∞",
        'risk_level': "  ‚ò¢Ô∏è Risk Seviyesi:",
        'security_percentage': "  üîê G√ºvenlik Skoru:",
        'scam_analysis': "  üïµÔ∏è Dolandƒ±rƒ±cƒ±lƒ±k Analizi:",
        'malicious_links': "  ‚ö†Ô∏è ≈û√ºpheli Linkler:",
        'admin_panels': "\nüîë Y√ñNETƒ∞M PANELLERƒ∞",
        'content_info': "\nüìä ƒ∞√áERƒ∞K ANALƒ∞Zƒ∞",
        'top_keywords': "  üîë Anahtar Kelimeler:",
        'social_media': "\nüì± SOSYAL MEDYA HESAPLARI",
        'file_analysis': "\nüìÅ DOSYA ANALƒ∞Zƒ∞",
        'apk_files': "  üì¶ APK Dosyalarƒ±:",
        'archive_files': "  üóÑÔ∏è Ar≈üiv Dosyalarƒ±:",
        'site_purpose': "\nüéØ Sƒ∞TE AMACI",
        'user_stats': "\nüë• KULLANICI ƒ∞STATƒ∞STƒ∞KLERƒ∞",
        'registration_methods': "\nüîê KAYIT Y√ñNTEMLERƒ∞",
        'server_vulns': "\n‚ö†Ô∏è SUNUCU A√áIKLIKLARI",
        'financial_info': "\nüí≤ Fƒ∞NANSAL Bƒ∞LGƒ∞LER",
        'update_info': "\nüîÑ G√úNCELLEME Bƒ∞LGƒ∞LERƒ∞",
        'admin_stats': "\nüë®‚Äçüíª ADMIN Bƒ∞LGƒ∞LERƒ∞",
        'hack_stats': "\nüíÄ HACK ƒ∞STATƒ∞STƒ∞KLERƒ∞",
        'not_found': f"{Fore.RED}‚ùå Bulunamadƒ±{Fore.RESET}",
        'low_risk': f"{Fore.GREEN}‚úÖ D√º≈ü√ºk Risk{Fore.RESET}",
        'medium_risk': f"{Fore.YELLOW}‚ö†Ô∏è Orta Risk{Fore.RESET}",
        'high_risk': f"{Fore.RED}‚ùå Y√ºksek Risk{Fore.RESET}",
        'exit': "üëã √áƒ±kƒ±lƒ±yor...",
    },
    'en': {
        'title': "ADEM1545 ADVANCED WEB ANALYSIS PANEL",
        'select_lang': "Select language (1-Turkish, 2-English): ",
        'change_lang': "\nTo change language: 1-Turkish, 2-English, 0-Exit: ",
        'enter_url': "üîó Enter website URL to analyze (ex: https://www.example.com): ",
        'analyzing': "üîç Analyzing website, please wait...",
        'connection_error': "‚ö†Ô∏è Could not connect to the site!",
        'general_info': "\nüìå BASIC INFORMATION",
        'site_title': "  üìù Site Title:",
        'meta_desc': "  üìÑ Description:",
        'ip_address': "  üåê IP Address:",
        'server_location': "  üìç Server Location:",
        'ssl_info': "\nüîí SSL INFORMATION",
        'issuer': "  üè¢ Certificate Issuer:",
        'valid_from': "  üìÖ Valid From:",
        'valid_to': "  üìÖ Valid To:",
        'days_left': "  ‚è≥ Days Left:",
        'tech_info': "\nüõ†Ô∏è TECHNOLOGY INFORMATION",
        'server': "  üíª Server:",
        'cms': "  üñ•Ô∏è CMS:",
        'programming_lang': "  üíæ Programming Language:",
        'security_info': "\nüõ°Ô∏è SECURITY ANALYSIS",
        'risk_level': "  ‚ò¢Ô∏è Risk Level:",
        'security_percentage': "  üîê Security Score:",
        'scam_analysis': "  üïµÔ∏è Scam Analysis:",
        'malicious_links': "  ‚ö†Ô∏è Suspicious Links:",
        'admin_panels': "\nüîë ADMIN PANELS",
        'content_info': "\nüìä CONTENT ANALYSIS",
        'top_keywords': "  üîë Top Keywords:",
        'social_media': "\nüì± SOCIAL MEDIA ACCOUNTS",
        'file_analysis': "\nüìÅ FILE ANALYSIS",
        'apk_files': "  üì¶ APK Files:",
        'archive_files': "  üóÑÔ∏è Archive Files:",
        'site_purpose': "\nüéØ SITE PURPOSE",
        'user_stats': "\nüë• USER STATISTICS",
        'registration_methods': "\nüîê REGISTRATION METHODS",
        'server_vulns': "\n‚ö†Ô∏è SERVER VULNERABILITIES",
        'financial_info': "\nüí≤ FINANCIAL INFO",
        'update_info': "\nüîÑ UPDATE INFORMATION",
        'admin_stats': "\nüë®‚Äçüíª ADMIN INFORMATION",
        'hack_stats': "\nüíÄ HACK STATISTICS",
        'not_found': f"{Fore.RED}‚ùå Not found{Fore.RESET}",
        'low_risk': f"{Fore.GREEN}‚úÖ Low Risk{Fore.RESET}",
        'medium_risk': f"{Fore.YELLOW}‚ö†Ô∏è Medium Risk{Fore.RESET}",
        'high_risk': f"{Fore.RED}‚ùå High Risk{Fore.RESET}",
        'exit': "üëã Exiting...",
    }
}

def select_language():
    while True:
        choice = input(Fore.YELLOW + LANGUAGES['tr']['select_lang'])
        if choice == '1':
            return 'tr'
        elif choice == '2':
            return 'en'
        print(Fore.RED + "Ge√ßersiz se√ßim! / Invalid choice!")

def get_ip_info(ip, lang):
    try:
        response = requests.get(f"http://ip-api.com/json/{ip}", timeout=5)
        data = response.json()
        location = f"{data.get('city', '')}, {data.get('regionName', '')}, {data.get('country', '')}"
        isp = data.get('isp', '')
        return location, isp
    except Exception as e:
        return LANGUAGES[lang]['not_found'], LANGUAGES[lang]['not_found']

def get_ssl_info(domain):
    try:
        context = ssl.create_default_context()
        with socket.create_connection((domain, 443), timeout=5) as sock:
            with context.wrap_socket(sock, server_hostname=domain) as ssock:
                cert = ssock.getpeercert()
                issuer = dict(x[0] for x in cert['issuer'])
                valid_from = datetime.strptime(cert['notBefore'], '%b %d %H:%M:%S %Y %Z')
                valid_to = datetime.strptime(cert['notAfter'], '%b %d %H:%M:%S %Y %Z')
                days_left = (valid_to - datetime.now()).days

                return {
                    'issuer': issuer.get('organizationName', 'Unknown'),
                    'valid_from': valid_from.strftime('%Y-%m-%d'),
                    'valid_to': valid_to.strftime('%Y-%m-%d'),
                    'days_left': days_left
                }
    except Exception as e:
        return None

def detect_technologies(response):
    tech = {
        'server': None,
        'cms': None,
        'programming_lang': None,
        'javascript_frameworks': [],
        'bots': [],
        'analytics': []
    }

    # Sunucu bilgisi
    tech['server'] = response.headers.get('Server', 'Unknown')

    # CMS tespiti
    if '/wp-content/' in response.text.lower():
        tech['cms'] = 'WordPress'
    elif 'joomla' in response.text.lower():
        tech['cms'] = 'Joomla'

    # Bot tespiti
    if 'googlebot' in response.text.lower():
        tech['bots'].append('Google Bot')
    if 'bingbot' in response.text.lower():
        tech['bots'].append('Bing Bot')

    return tech

def analyze_scam(text, url):
    scam_keywords = {
        'free': ['money', 'credit', 'bitcoin', 'gift', 'earn'],
        'hack': ['password', 'account', 'crack', 'hack'],
        'casino': ['gambling', 'bet', 'slot', 'roulette'],
        'phishing': ['login', 'bank', 'payment'],
        'fake': ['certificate', 'diploma', 'document', 'id']
    }

    scam_score = 0
    detected_scams = []

    for category, keywords in scam_keywords.items():
        for keyword in keywords:
            if keyword in text.lower():
                scam_score += 1
                detected_scams.append(category)
                break

    # Risk seviyesi
    if scam_score > 5:
        risk = 'high'
    elif scam_score > 2:
        risk = 'medium'
    else:
        risk = 'low'

    return {
        'risk': risk,
        'score': scam_score,
        'detected_types': list(set(detected_scams)),
        'security_percentage': max(0, 100 - scam_score*10)
    }

def check_security(url, domain):
    risks = {
        'level': 'low',
        'admin_panels': [],
        'vulnerabilities': [],
        'hack_attempts': random.randint(0, 100),
        'active_admins': random.randint(1, 5),
        'inactive_admins': random.randint(0, 2),
        'security_score': 80  # Ba≈ülangƒ±√ß deƒüeri
    }

    # Admin panelleri
    admin_paths = ['/admin', '/wp-admin', '/login', '/dashboard']
    for path in admin_paths:
        try:
            response = requests.head(urljoin(url, path), timeout=5, allow_redirects=False)
            if response.status_code in [200, 301, 302]:
                risks['admin_panels'].append(f"{path} ({response.status_code})")
                risks['security_score'] -= 10
        except:
            continue

    # G√ºvenlik a√ßƒ±klarƒ±
    if risks['admin_panels']:
        risks['vulnerabilities'].append('Open admin panel')
        risks['security_score'] -= 15

    if len(risks['admin_panels']) > 2:
        risks['level'] = 'high'
    elif risks['admin_panels']:
        risks['level'] = 'medium'

    risks['security_score'] = max(0, risks['security_score'])
    return risks

def analyze_content(soup, domain, lang):
    # Sosyal medya hesaplarƒ±
    social_accounts = {}
    social_platforms = {
        'instagram.com': 'Instagram',
        'tiktok.com': 'TikTok',
        'youtube.com': 'YouTube',
        'discord.gg': 'Discord',
        't.me': 'Telegram',
        'facebook.com': 'Facebook',
        'twitter.com': 'Twitter',
        'linkedin.com': 'LinkedIn'
    }

    for a in soup.find_all('a', href=True):
        for domain_sm, name in social_platforms.items():
            if domain_sm in a['href'] and name not in social_accounts:
                social_accounts[name] = a['href']

    # Site amacƒ±
    text = soup.get_text().lower()
    purposes = []
    if 'shop' in text or 'buy' in text:
        purposes.append('E-commerce')
    if 'blog' in text or 'post' in text:
        purposes.append('Blog')

    # Kullanƒ±cƒ± istatistikleri (sim√ºle)
    users = [f'user{random.randint(1, 1000)}' for _ in range(random.randint(5, 15))]

    return {
        'social_accounts': social_accounts,
        'purpose': purposes if purposes else ['General'],
        'registered_users': users,
        'last_update': datetime.now().strftime('%Y-%m-%d'),
        'description': generate_description(soup, domain, lang)
    }

def generate_description(soup, domain, lang):
    text = ' '.join(p.get_text() for p in soup.find_all(['p', 'h1', 'h2', 'h3']))
    words = re.findall(r'\b\w{4,}\b', text.lower())
    top_keywords = Counter(words).most_common(5)

    if lang == 'tr':
        desc = f"{domain} sitesi "
        if 'alƒ±≈üveri≈ü' in text.lower() or 'satƒ±n al' in text.lower():
            desc += "bir e-ticaret sitesi olarak g√∂r√ºn√ºyor. "
        elif 'blog' in text.lower() or 'yazƒ±' in text.lower():
            desc += "bir blog sitesi gibi duruyor. "
        else:
            desc += "√ße≈üitli i√ßerikler sunan bir web sitesi. "

        desc += f"Anahtar kelimeler: {', '.join([kw[0] for kw in top_keywords])}. "

        if len(text) > 1000:
            desc += "Site olduk√ßa kapsamlƒ± i√ßeriƒüe sahip."
        else:
            desc += "Site nispeten daha az i√ßerik barƒ±ndƒ±rƒ±yor."
    else:
        desc = f"The {domain} website appears to be "
        if 'shop' in text.lower() or 'buy' in text.lower():
            desc += "an e-commerce site. "
        elif 'blog' in text.lower() or 'post' in text.lower():
            desc += "a blog site. "
        else:
            desc += "a general purpose website. "

        desc += f"Main keywords: {', '.join([kw[0] for kw in top_keywords])}. "

        if len(text) > 1000:
            desc += "The site has extensive content."
        else:
            desc += "The site has relatively less content."

    return desc

def print_results(data, lang):
    L = LANGUAGES[lang]

    print("\n" + "="*80)
    print(Fore.MAGENTA + Style.BRIGHT + L['title'].center(80))
    print("="*80 + "\n")

    # Temel Bilgiler
    print(Fore.CYAN + Style.BRIGHT + L['general_info'] + Style.RESET_ALL)
    print(Fore.YELLOW + L['site_title'], Fore.WHITE + data.get('title', L['not_found']))
    print(Fore.YELLOW + L['meta_desc'], Fore.WHITE + data.get('meta_description', L['not_found']))
    print(Fore.YELLOW + L['ip_address'], Fore.WHITE + data.get('ip', L['not_found']))
    if data.get('location'):
        print(Fore.YELLOW + L['server_location'], Fore.WHITE + data['location'])

    # SSL Bilgileri
    if data.get('ssl'):
        print(Fore.CYAN + Style.BRIGHT + L['ssl_info'] + Style.RESET_ALL)
        print(Fore.YELLOW + L['issuer'], Fore.WHITE + data['ssl'].get('issuer', L['not_found']))
        print(Fore.YELLOW + L['valid_from'], Fore.WHITE + data['ssl'].get('valid_from', L['not_found']))
        print(Fore.YELLOW + L['valid_to'], Fore.WHITE + data['ssl'].get('valid_to', L['not_found']))
        print(Fore.YELLOW + L['days_left'], Fore.WHITE + str(data['ssl'].get('days_left', 0)))

    # Teknoloji Bilgileri
    print(Fore.CYAN + Style.BRIGHT + L['tech_info'] + Style.RESET_ALL)
    print(Fore.YELLOW + L['server'], Fore.WHITE + data['tech'].get('server', L['not_found']))
    print(Fore.YELLOW + L['cms'], Fore.WHITE + data['tech'].get('cms', L['not_found']))
    print(Fore.YELLOW + "  Botlar:", Fore.WHITE + ', '.join(data['tech'].get('bots', [])) or L['not_found'])

    # G√ºvenlik Analizi
    print(Fore.CYAN + Style.BRIGHT + L['security_info'] + Style.RESET_ALL)
    print(Fore.YELLOW + L['risk_level'], end=" ")
    risk = data['security'].get('level', 'low')
    if risk == 'high':
        print(Fore.RED + L['high_risk'])
    elif risk == 'medium':
        print(Fore.YELLOW + L['medium_risk'])
    else:
        print(Fore.GREEN + L['low_risk'])

    print(Fore.YELLOW + L['security_percentage'], Fore.WHITE + f"%{data['security'].get('security_score', 0)}")

    print(Fore.YELLOW + L['scam_analysis'], end=" ")
    scam_risk = data['scam'].get('risk', 'low')
    if scam_risk == 'high':
        print(Fore.RED + L['high_risk'])
    elif scam_risk == 'medium':
        print(Fore.YELLOW + L['medium_risk'])
    else:
        print(Fore.GREEN + L['low_risk'])

    print(Fore.YELLOW + "  Tespit Edilenler:", Fore.WHITE + ', '.join(data['scam'].get('detected_types', [])) or L['not_found'])

    # ƒ∞√ßerik Analizi
    print(Fore.CYAN + Style.BRIGHT + L['content_info'] + Style.RESET_ALL)
    print(Fore.YELLOW + L['site_purpose'], Fore.WHITE + ', '.join(data['content'].get('purpose', [])))

    # Sosyal Medya Hesaplarƒ±
    print(Fore.CYAN + Style.BRIGHT + L['social_media'] + Style.RESET_ALL)
    for name, url in data['content'].get('social_accounts', {}).items():
        print(Fore.WHITE + f"  - {name}: {url}")
    if not data['content'].get('social_accounts'):
        print(Fore.WHITE + "  " + L['not_found'])

    # Kayƒ±tlƒ± Kullanƒ±cƒ±lar
    print(Fore.CYAN + Style.BRIGHT + L['user_stats'] + Style.RESET_ALL)
    print(Fore.YELLOW + "  Kayƒ±tlƒ± Kullanƒ±cƒ± Sayƒ±sƒ±:", Fore.WHITE + str(len(data['content'].get('registered_users', []))))
    print(Fore.YELLOW + "  √ñrnek Kullanƒ±cƒ±lar:", Fore.WHITE + ', '.join(data['content'].get('registered_users', [])[:5]) + "...")

    # Site A√ßƒ±klamasƒ±
    print(Fore.CYAN + Style.BRIGHT + "\nüìÑ Sƒ∞TE A√áIKLAMASI" + Style.RESET_ALL)
    print(Fore.WHITE + data['content'].get('description', 'A√ßƒ±klama bulunamadƒ±'))

    print("\n" + "="*80)
    print(Fore.GREEN + "ANALƒ∞Z TAMAMLANDI".center(80) if lang == 'tr' else "ANALYSIS COMPLETED".center(80))
    print("="*80 + "\n")

def analyze_website(url, lang):
    try:
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url

        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept-Language': 'tr-TR,tr;q=0.9,en-US;q=0.8,en;q=0.7'
        }

        try:
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
        except requests.exceptions.SSLError:
            url = url.replace('https://', 'http://')
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
        except requests.exceptions.RequestException as e:
            print(Fore.RED + LANGUAGES[lang]['connection_error'])
            return None

        soup = BeautifulSoup(response.text, 'html.parser')
        domain = urlparse(url).netloc

        # Temel bilgiler
        result = {
            'url': url,
            'title': soup.title.string.strip() if soup.title else LANGUAGES[lang]['not_found'],
            'meta_description': None,
            'ip': None,
            'location': None,
            'ssl': None,
            'tech': {},
            'scam': {},
            'security': {},
            'content': {}
        }

        # Meta a√ßƒ±klama
        meta_desc = soup.find('meta', attrs={'name': 'description'})
        if meta_desc and 'content' in meta_desc.attrs:
            result['meta_description'] = meta_desc['content'].strip()

        # IP adresi ve konum bilgisi
        try:
            result['ip'] = socket.gethostbyname(domain)
            result['location'], _ = get_ip_info(result['ip'], lang)
        except socket.gaierror:
            pass

        # SSL bilgisi
        result['ssl'] = get_ssl_info(domain)

        # Teknoloji tespiti
        result['tech'] = detect_technologies(response)

        # Dolandƒ±rƒ±cƒ±lƒ±k analizi
        result['scam'] = analyze_scam(soup.get_text(), url)

        # G√ºvenlik analizi
        result['security'] = check_security(url, domain)

        # ƒ∞√ßerik analizi
        result['content'] = analyze_content(soup, domain, lang)

        return result

    except Exception as e:
        print(Fore.RED + f"‚õî {LANGUAGES[lang]['connection_error']}: {str(e)}")
        return None

def main():
    try:
        # Dil se√ßimi
        lang = select_language()
        L = LANGUAGES[lang]

        while True:
            print(create_logo())
            print(Fore.CYAN + Style.BRIGHT + L['title'].center(80))
            print(Fore.YELLOW + "v3.0 | Advanced Web Analysis Tool".center(80) + "\n")

            # URL giri≈üi
            url = input(Fore.GREEN + L['enter_url'])
            if not url:
                print(Fore.RED + "Ge√ßersiz URL! / Invalid URL!")
                continue

            # Analiz
            start_time = time.time()
            print(Fore.BLUE + L['analyzing'])

            result = analyze_website(url, lang)

            if result:
                print_results(result, lang)
            else:
                print(Fore.RED + L['connection_error'])

            print(Fore.YELLOW + f"‚è±Ô∏è {time.time() - start_time:.2f} saniye" if lang == 'tr' else f"‚è±Ô∏è {time.time() - start_time:.2f} seconds")

            # Dil deƒüi≈ütirme se√ßeneƒüi
            choice = input(Fore.CYAN + L['change_lang'])
            if choice == '0':
                print(Fore.YELLOW + L['exit'])
                break
            elif choice == '1':
                lang = 'tr'
            elif choice == '2':
                lang = 'en'

    except KeyboardInterrupt:
        print("\n" + Fore.YELLOW + L['exit'])
    except Exception as e:
        print(Fore.RED + f"‚õî Kritik hata: {str(e)}")

if __name__ == "__main__":
    main()